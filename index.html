<p><img src="assets/images/ArrowArcLogo.png" alt="Alt text"></p>
<p><strong>ArrowArc</strong> is a high-performance data integration platform designed to challenge the status quo in big
    data processing. At its core, ArrowArc is built on the principle that <strong>&quot;small data is the new big
        data&quot;</strong>—a recognition that for the vast majority of use cases, traditional Big Data tools, including
    modern data warehouses like Snowflake or BigQuery, are not always necessary.</p>
<p>ArrowArc provides an alternative approach to data exchange between platforms and systems, focusing on speed,
    efficiency, and optimal resource utilization. This platform acknowledges that &quot;small data&quot; is not strictly
    about volume but about intelligently leveraging data processing tools that are better suited for many real-world
    applications.</p>
<p>Leveraging the power of :bow_and_arrow: <strong>Apache Arrow</strong>, Go, and other cutting-edge technologies,
    ArrowArc enables rapid data movement between various sources and sinks. The platform is designed with a singular
    focus on breakneck speed and simplification of complex data workflows, abstracting away the intricacies of modern
    data integration while leveraging the capabilities of modern hardware.</p>
<p>By shifting the focus from merely handling large-scale data to optimizing the performance of smaller, more targeted
    datasets, ArrowArc makes pseudo-realtime data integration both feasible and cost-effective. This allows
    organizations to sync data at frequent intervals—every hour, 10 minutes, or whatever their needs dictate—without the
    prohibitive costs associated with true streaming solutions like Dataflow at scale.</p>
<p>Whether you&#39;re dealing with cloud storage, relational databases, or diverse file formats, ArrowArc offers a
    streamlined, high-performance solution that prioritizes efficiency and ease of use. This makes it the ideal platform
    for modern data integration challenges, especially for those looking to embrace the small data revolution.</p>
<hr>
<h2 id="overview">Overview</h2>
<p>ArrowArc is architected to handle complex data integration tasks with a focus on extreme performance, low latency,
    and high throughput. At the heart of ArrowArc&#39;s design is a commitment to efficient, concurrent data
    processing—an area where Go excels due to its built-in support for concurrency and its lightweight goroutines.</p>
<h3 id="go-channels-for-data-streaming">Go Channels for Data Streaming</h3>
<p>The core of ArrowArc is built around a common pattern for data extraction and ingestion, utilizing Go channels as the
    interface for streaming data between various sources and sinks:</p>
<pre><code class="lang-go">&lt;-<span class="hljs-keyword">chan</span> arrow.Record, &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">error</span>
</code></pre>
<h3 id="why-go-channels-">Why Go Channels?</h3>
<p>Go channels are a powerful concurrency primitive that enable safe and efficient communication between goroutines. In
    the context of ArrowArc, channels provide a natural and elegant way to stream data (in the form of Apache Arrow
    <code>Record</code> objects) between different components of the system.</p>
<ul>
    <li>
        <p><strong>Concurrency and Parallelism</strong>: Go&#39;s concurrency model allows multiple tasks to run
            concurrently, facilitating efficient parallel data processing. By using channels, ArrowArc can seamlessly
            manage data flow between producer and consumer goroutines, ensuring that data is processed as quickly as it
            becomes available.</p>
    </li>
    <li>
        <p><strong>Decoupling Data Flow</strong>: Channels decouple the production and consumption of data. Producers
            (such as data extraction routines) can send data to a channel as soon as it&#39;s ready, without needing to
            wait for consumers (such as data ingestion routines) to be available. This decoupling enhances resource
            utilization and throughput, allowing both sides to operate independently.</p>
    </li>
    <li>
        <p><strong>Error Handling</strong>: By returning a second channel specifically for errors, ArrowArc ensures
            clean and structured error handling. This approach allows any issues during data processing to be
            communicated back to the caller without interrupting the data flow.</p>
    </li>
    <li>
        <p><strong>Low Latency and High Throughput</strong>: Go channels, in combination with goroutines, minimize the
            overhead associated with data transfer between system components. This leads to lower latency in data
            processing and higher overall throughput, enabling continuous and efficient data streaming.</p>
    </li>
</ul>
<h2 id="intent-of-the-architecture">Intent of the Architecture</h2>
<p>The architecture of ArrowArc is designed to expose a flexible and efficient pattern for both data extraction and
    ingestion, where all integrations (sources and sinks) follow a common interface using channels to stream Apache
    Arrow records. This approach ensures that data flows through the system with minimal overhead, maintaining the
    high-speed processing capabilities of Apache Arrow while leveraging the concurrency features of Go.</p>
<p>By adhering to this common pattern, ArrowArc demonstrates how different components can be integrated into a cohesive
    system, making it easier to extend and adapt to new data formats and storage solutions.</p>
<hr>
<h3 id="key-features">Key Features</h3>
<ul>
    <li><strong>Common Extraction and Ingestion Pattern</strong>: All integrations leverage a standardized approach
        using a common interface for both input and output, simplifying the development and extension of connectors and
        sinks.</li>
    <li><strong>Streaming Data Integration</strong>: Efficiently stream data between sources and sinks with minimal
        overhead.</li>
    <li><strong>Support for Multiple Formats</strong>: Write data to Parquet, CSV, and more, directly to destinations
        like Google Cloud Storage (GCS).</li>
    <li><strong>Go-Based Architecture</strong>: Built entirely in Go, ArrowArc maximizes performance, scalability, and
        concurrency management.</li>
    <li><strong>Apache Arrow Integration</strong>: Utilizing Apache Arrow for in-memory columnar data processing,
        ArrowArc ensures high-speed data handling and interoperability between systems.</li>
    <li><strong>Extensible and Modular Design</strong>: Easily extend ArrowArc to support additional data formats or
        cloud storage providers by following the established patterns.</li>
</ul>
<h2 id="components">Components</h2>
<h3 id="integrations">Integrations</h3>
<p>ArrowArc is engineered with a singular focus: achieving breakneck speed and simplifying data integration workflows
    centered around Apache Arrow. By abstracting away much of the inherent complexity, ArrowArc enables developers to
    integrate with various platforms, storage providers, and data formats, both for extraction and ingestion purposes.
</p>
<h4 id="example-google-cloud-bigquery">Example: Google Cloud BigQuery</h4>
<pre><code class="lang-go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(bq *BigQueryConnector)</span> <span class="hljs-title">GetBigQueryArrowStream</span><span class="hljs-params">(ctx context.Context, projectID <span class="hljs-keyword">string</span>, datasetID <span class="hljs-keyword">string</span>, tableID <span class="hljs-keyword">string</span>, format <span class="hljs-keyword">string</span>)</span> <span class="hljs-params">(&lt;-<span class="hljs-keyword">chan</span> arrow.Record, &lt;-<span class="hljs-keyword">chan</span> error)</span></span> {
}
</code></pre>
<h3 id="sinks">Sinks</h3>
<p>Data can be streamed and written to various sinks, such as Google Cloud Storage and Postgres. ArrowArc supports
    writing data in formats like Parquet and CSV or platforms like DuckDB, with more integrations coming.</p>
<h4 id="example-google-cloud-storage">Example: Google Cloud Storage</h4>
<pre><code class="lang-go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *GCSSink)</span> <span class="hljs-title">WriteGCSArrowStream</span><span class="hljs-params">(ctx context.Context, records &lt;-<span class="hljs-keyword">chan</span> arrow.Record, filePath <span class="hljs-keyword">string</span>, format FileFormat, delimiter ...<span class="hljs-keyword">rune</span>)</span> &lt;-<span class="hljs-title">chan</span> <span class="hljs-title">error</span></span> {
}
</code></pre>
<hr>
<h3 id="gluing-it-all-together">Gluing it All Together</h3>
<p>ArrowArc simplifies high-performance data synchronization, enabling tasks like rewriting a Parquet file with minimal
    code.</p>
<pre><code class="lang-go"><span class="hljs-comment">// Stream data from a Parquet file using a memory map in 1,000,000 record batches</span>
recordChan, errChan := GetParquetArrowStream(ctx, inFilePath, <span class="hljs-literal">true</span>, <span class="hljs-number">1000000</span>)

<span class="hljs-comment">// Handle errors</span>
<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> {
    <span class="hljs-keyword">if</span> err := &lt;-errChan; err != <span class="hljs-literal">nil</span> {
        log.Fatalf(<span class="hljs-string">"Error streaming from Parquet file: %v"</span>, err)
    }
}()

<span class="hljs-comment">// Write data to an output Parquet file</span>
<span class="hljs-keyword">if</span> err := WriteFileParquet(ctx, outputFilePath, recordChan); err != <span class="hljs-literal">nil</span> {
    log.Fatalf(<span class="hljs-string">"Error writing to output Parquet file: %v"</span>, err)
}
</code></pre>
<p>In just a few lines of code, ArrowArc can stream data from a Parquet file and write it back out, all while leveraging
    the power of Apache Arrow for in-memory data processing. This streamlined approach allows you to focus on building
    effective solutions without getting entangled in the complexities of data handling.</p>
<p>Additionally, ArrowArc makes it easy to read data once from a source and write it to multiple sinks efficiently.</p>
<pre><code class="lang-go">recordChan, errChan := GetBigQueryArrowStream(ctx)

<span class="hljs-keyword">var</span> wg sync.WaitGroup

<span class="hljs-comment">// Setup multiple sinks</span>
sinks := []<span class="hljs-keyword">string</span>{<span class="hljs-string">"GCS[parquet]"</span>, <span class="hljs-string">"File[avro]"</span>, <span class="hljs-string">"Postgres"</span>}
<span class="hljs-keyword">for</span> _, sink := <span class="hljs-keyword">range</span> sinks {
    wg.Add(<span class="hljs-number">1</span>)
    <span class="hljs-keyword">go</span> WriteToSink(ctx, sink, recordChan, &amp;wg)
}

<span class="hljs-comment">// Error handling goroutine</span>
<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> {
    <span class="hljs-keyword">for</span> err := <span class="hljs-keyword">range</span> errChan {
        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
            log.Printf(<span class="hljs-string">"Error occurred: %v"</span>, err)
            cancel() <span class="hljs-comment">// Cancel all operations if an error occurs</span>
        }
    }
}()

wg.Wait() <span class="hljs-comment">// Wait for all sinks to finish processing</span>
</code></pre>
<hr>
<h3 id="arrowarc-feature-matrix">ArrowArc Feature Matrix</h3>
<p>This tables below indicate the status of the planned features of ArrowArc, including command line utilities,
    integrations, and cloud storage provider support. The status of each feature is indicated as follows:</p>
<ul>
    <li><code>✅</code> - Implemented</li>
    <li><code>🚧</code> - In Progress</li>
    <li><code>❌</code> - Not Started</li>
</ul>
<h3 id="features-overview">Features Overview</h3>
<h4 id="command-line-utilities">Command Line Utilities</h4>
<table>
    <thead>
        <tr>
            <th>Utility</th>
            <th>Status</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>Extract and Ingest</strong></td>
            <td>🚧</td>
        </tr>
        <tr>
            <td><strong>Rewrite Parquet</strong></td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>Convert CSV</strong></td>
            <td>🚧</td>
        </tr>
        <tr>
            <td><strong>Sync Table</strong></td>
            <td>❌</td>
        </tr>
    </tbody>
</table>
<hr>
<h4 id="integration-types">Integration Types</h4>
<h5 id="1-database-integrations-">1. <strong>Database Integrations</strong></h5>
<table>
    <thead>
        <tr>
            <th>Database</th>
            <th>Extraction</th>
            <th>Ingestion</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>PostgreSQL</strong></td>
            <td>✅</td>
            <td>🚧</td>
        </tr>
        <tr>
            <td><strong>MySQL</strong></td>
            <td>🚧</td>
            <td>❌</td>
        </tr>
        <tr>
            <td><strong>Oracle</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
        <tr>
            <td><strong>BigQuery</strong></td>
            <td>✅</td>
            <td>🚧</td>
        </tr>
        <tr>
            <td><strong>Snowflake</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
        <tr>
            <td><strong>DuckDB</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>SQLite</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
    </tbody>
</table>
<h5 id="2-cloud-storage-integrations-">2. <strong>Cloud Storage Integrations</strong></h5>
<table>
    <thead>
        <tr>
            <th>Provider</th>
            <th>Extraction</th>
            <th>Ingestion</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>Google Cloud Storage (GCS)</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>Amazon S3</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
        <tr>
            <td><strong>Azure Blob Storage</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
    </tbody>
</table>
<h5 id="3-filesystem-formats-">3. <strong>Filesystem Formats</strong></h5>
<table>
    <thead>
        <tr>
            <th>Format</th>
            <th>Extraction</th>
            <th>Ingestion</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>Parquet</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>Avro</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
        <tr>
            <td><strong>CSV</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>JSON</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>IPC</strong></td>
            <td>✅</td>
            <td>✅</td>
        </tr>
        <tr>
            <td><strong>Iceberg</strong></td>
            <td>❌</td>
            <td>❌</td>
        </tr>
    </tbody>
</table>
<hr>
<h3 id="arrow-flight-note">Arrow Flight Note</h3>
<p>Apache Arrow Flight is a groundbreaking initiative that introduces a high-performance wire protocol designed for
    large-scale data transfers. It leverages the Apache Arrow in-memory columnar format to enable faster and more
    efficient data exchange between systems, particularly in the context of modern, distributed data environments. Arrow
    Flight&#39;s ability to reduce serialization and deserialization overhead makes it a compelling choice for real-time
    data operations and integration.</p>
<p>ArrowArc fully recognizes the potential of Arrow Flight and will support it as part of its broader data exchange
    capabilities. However, it’s important to note that Arrow Flight defines a wire protocol that needs to be implemented
    by database vendors. While Apache Arrow itself has firmly established its place in the data processing ecosystem,
    the adoption and longevity of Arrow Flight remain uncertain. The success of Arrow Flight largely depends on its
    adoption by database vendors and the broader data community.</p>
<p>This uncertainty was a key motivator behind the creation of ArrowArc. ArrowArc seeks to provide a flexible and
    efficient data exchange platform that does not rely solely on the adoption of a specific protocol like Arrow Flight.
    Instead, ArrowArc focuses on speed, efficiency, and broad compatibility with existing systems, while embracing the
    full potential of the Apache Arrow ecosystem as it relates to data exchange. This approach ensures that
    organizations can optimize their data integration processes today, while remaining adaptable to future developments
    in the Arrow ecosystem.</p>
<hr>
<h2 id="-page_facing_up-license">:page_facing_up: License</h2>
<p>Please see the <a href="./LICENSE">LICENSE</a> for more details.</p>
<h2 id="author">Author</h2>
<p><a href="https://github.com/TFMV">Thomas F McGeehan V</a></p>